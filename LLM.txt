1. 大模型的prompt如何设计有什么心得或技巧？
基本框架：RTF框架 role+task+format   思维链框架 “让我们逐步思考” RISEN框架 role+instruction+step+end goal + narrowing
两大核心原则：1. 编写明确和具体的指令 2. 给予模型充足的思考时间
几大策略：1. 巧妙使用分隔符 2. 要求结构化输出（一般是json） 3. few shot prompting 4. 明确完成任务需要的步骤 5. 引导模型在得出结论前充分思考方案

2. 大模型产生幻觉指的是什么？如何避免或者说减少幻觉呢？
即使模型在训练过程中接触了海量知识，但它并不能完美记住所有信息，也难以精准把握自身知识的边界。这就导致在面对晦涩主题的问题时，模型可能会编造出看似合理但实际上错误的内容，这就是所谓的“幻觉”。
内在幻觉：生成的内容与源内容相互矛盾
外在幻觉：生成的内容无法从源内容中验证，既可能正确也可能错误。

幻觉的原因：
数据驱动原因：训练数据中源与参考的不匹配可能导致幻觉，如数据对不对齐，导致生成不忠实的文本。
表示和解码的不完善：编码器理解能力的缺陷和解码器策略错误可能导致幻觉。解码器可能关注错误的输入部分，或使用增加幻觉风险的策略，例如基于采样的解码中的随机性。
参数知识偏见：预训练模型可能偏好其参数中的知识而非新输入，从而导致幻觉。解决幻觉现象的方法

幻觉解决方法：构建忠实数据集：手动创建含准确目标的数据集，或用标注员标记生成的数据中的幻觉。这种方法针对特定任务，可能资源密集型。
自动清洗数据：在实例级别过滤掉幻觉内容，或根据参考修正输入数据，特别适用于结构化数据到文本任务。
信息增强：用外部信息增强输入数据，更好地与目标对齐，提高模型的语义理解能力，减少与源的偏离。
模型和推理技术：
改进编码器和注意力机制：修改编码器架构和注意力机制，更多关注源数据，提高表示学习。
改进解码器：使用多分支、意识到不确定性或受限的解码器来减少幻觉的可能性。
训练方法：规划/草图：实施单独的规划步骤或将其整合到模型中，控制内容生成。强化学习：使用奖励优化模型，以减少幻觉。
多任务学习：同时在多个任务上训练，提高模型的泛化能力，减少对单一数据集的依赖。
可控生成：使用技术控制输出中的幻觉程度，适应不同的应用需求。
后处理：使用生成-然后-精炼策略纠正输出中的幻觉，特别适用于嘈杂的数据集。


3. 除了Lora还有什么SFT方法？
AdapterTuning-- 设计了Adapter 结构，将其嵌入 Transformer 的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数）
prefix tuning -- 与Full-finetuning 更新所有参数的方式不同，该方法是在输入 token 之前构造一段任务相关的 virtual tokens 作为 Prefix，然后训练的时候只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定。同时，为了防止直接更新 Prefix 的参数导致训练不稳定的情况，他们在 Prefix 层前面加了 MLP 结构(相当于将Prefix 分解为更小维度的 Input 与 MLP 的组合后输出的结果)，训练完成后，只保留 Prefix 的参数。

4. 为什么用Adam不用SGD？
Adam 对每个参数单独计算自适应学习率，可以适应不同梯度大小的参数更新，而 SGD 需要手动调整学习率。Adam 适合高维、稀疏和复杂的优化问题。Adam 收敛速度比 SGD 快。大模型对训练稳定性要求更高，SGD 容易出现梯度爆炸或梯度消失，特别是在 Transformer 这种深层网络中，Adam 的**梯度缩放（梯度平方的滑动平均）**有助于稳定训练。SGD 需要更大 batch size 才能稳定训练，而大模型可能受 显存限制，Adam 在 小 batch size 下仍能稳定优化。Adam 通过自适应学习率调整，可以更好地适应数据并行（Data Parallel）或模型并行（Model Parallel）环境，减少人工调整超参数的需求。

5. 大模型一般使用的是什么分词方式？你还知道哪些分词方式？简单描述一下BPE算法？
一般用子词级分词 还有字符级和词级、字节级
大多数大模型（如 GPT-4、LLaMA、BERT）使用**子词（Subword Tokenization）**方法进行分词，以兼顾 泛化能力、计算效率和词表大小。常见的子词分词方法包括：
BPE（Byte Pair Encoding） → GPT-2、GPT-3、LLaMA 使用
WordPiece → BERT、RoBERTa 使用
Unigram Language Model（ULM） → T5、XLNet、ALBERT 使用
SentencePiece → GPT-4、T5 变种、mT5、BigScience Bloom 使用
BPE（Byte Pair Encoding） 是一种基于统计的子词分词方法，核心思想是：
通过合并高频的字符或子词对，逐步构建子词词典，直到达到预设的词汇表大小。

BPE 主要步骤
初始化：将文本中的所有单词拆成单字符（或者 byte 级别的字符，如 GPT-2）。
统计字符/子词对的频率：计算所有相邻字符（子词）对的出现次数。
合并最高频的子词对：选取频率最高的相邻子词对，将它们合并为一个新子词。
重复步骤 2-3，直到达到预设的词汇大小（Vocabulary Size）。
最终，所有文本用子词表示，减少 OOV（未登录词）问题。

6. 什么是3D并行？DP和DDP是什么？
3D 并行（Three-Dimensional Parallelism） 是深度学习大模型训练中的一种并行策略，它结合了以下三种并行方式：
数据并行（Data Parallelism, DP）每个 GPU 处理不同的训练样本，参数是相同的。适用于中等规模模型，但受限于显存大小，无法训练超大模型。
张量并行（Tensor Parallelism, TP）将单个神经网络层的计算拆分到多个 GPU 上，每个 GPU 计算部分的张量运算。适用于超大模型，因为可以分摊每一层的计算和显存需求。
流水线并行（Pipeline Parallelism, PP）将整个模型的不同层拆分到不同的 GPU 上，每个 GPU 负责部分层的前向和反向计算。适用于超深模型，减少显存占用，但可能有**管道等待（bubble）**问题。

📌 3D 并行 = 数据并行（DP） + 张量并行（TP） + 流水线并行（PP）

DP仅支持单机多 GPU，主 GPU 负责梯度聚合（单主进程，通信瓶颈）    DDP 适用于多机多 GPU 各 GPU 直接梯度同步（all-reduce）


7. RAG是什么？RAG对于表格和图片数据如何处理？向量检索和召回怎么做的？
RAG 是一种结合检索和生成的技术，通过利用外部知识库提升生成模型的回答质量。

对于表格数据，常常需要先将结构化数据转换为文本描述或使用专门的表格编码器；
对于图片数据，则可以通过视觉特征提取或生成文本描述来处理。

向量检索 则依赖于将数据编码为向量、建立向量索引、计算向量相似度以及召回最相关的信息，再辅助生成最终输出。


8. vllm
vLLM的核心创新在于其PagedAttention机制，这是一种内存管理技术，灵感来自操作系统中的分页技术。这一机制带来了几个关键优势：
高效的KV缓存管理：在LLM推理过程中，每生成一个新token都需要保存之前所有token的注意力键值(KV)缓存。对于长上下文或多用户场景，这会导致巨大的内存压力。PagedAttention将KV缓存分块存储和管理，实现了更灵活的内存分配。
持续批处理：vLLM允许在生成过程中动态添加和移除请求，而不需要等待整个批次完成。这在服务部署中特别有用，因为它大幅提高了GPU利用率。
推理优化：vLLM包含了多项针对LLM推理的优化技术，包括张量并行、量化支持和高效的注意力计算。

9.bert和llm的位置编码有什么不同？为什么要位置编码？



FSDP的核心思想是将模型参数、梯度和优化器状态分片存储在多个设备上，而不是像传统数据并行那样在每个设备上复制完整模型。具体来说，FSDP结合了数据并行和模型并行的优势：

参数分片：模型的参数被均匀地分配到所有参与训练的GPU上，每个GPU只存储一部分参数。
计算时重组：在前向传播和反向传播的计算阶段，FSDP会临时将所需的参数从其他GPU收集到当前GPU，完成计算后再释放这部分内存。
梯度分片与同步：在反向传播过程中，每个GPU计算自己分片参数的梯度，然后通过集体通信操作（如All-Reduce）在所有GPU间同步梯度。
优化器状态分片：与参数相对应的优化器状态（如Adam优化器中的动量和方差）也被分片存储。





跨模态对齐技术详解
跨模态对齐(cross-modal alignment)是多模态人工智能的核心技术，它解决了不同模态数据（如文本、图像、音频）之间建立关联和映射的问题。随着大型多模态模型的兴起，对齐技术也在不断演进。让我详细介绍各种对齐方法和Q-Former逐渐被替代的原因。
主要的跨模态对齐方法
1. 对比学习（Contrastive Learning）
原理：通过最大化相关数据对（如图像和对应描述）的相似度，同时最小化不相关数据对的相似度。
代表方法：

CLIP (Contrastive Language-Image Pre-training)：使用大规模图文对，通过对比学习使文本和图像特征在共享空间中对齐。
ALIGN：使用更大规模但更嘈杂的网络图文对数据集，采用双塔结构进行对比学习。

特点：简单高效，适合大规模预训练，但可能存在模态间细粒度对齐不足的问题。
2. 注意力机制（Attention Mechanism）
原理：使用注意力机制在不同模态特征之间建立动态连接。
代表方法：

ViLBERT/LXMERT：使用双流注意力结构，先分别处理单模态信息，再通过交叉注意力进行模态融合。
ALBEF：结合了对比学习和注意力对齐，引入了多模态注意力融合模块。

特点：能够捕捉模态间的细粒度关系，但计算复杂度较高。
3. 编码器-解码器架构（Encoder-Decoder）
原理：一个模态的编码结果作为另一个模态解码器的输入或条件。
代表方法：

DALL-E/CogView：从文本生成图像，文本编码结果引导图像生成。
BEiT-3：使用统一的编码器-解码器处理多种模态。

特点：灵活性强，适合跨模态生成任务，需要更多的计算资源。
4. 统一表征（Unified Representation）
原理：将不同模态的信息映射到统一的潜在空间。
代表方法：

FLAVA：使用统一的Transformer结构处理不同模态，学习共享表征。
BLIP-2：通过引入冻结的语言模型和视觉模型，学习统一的跨模态表征。

特点：易于扩展到多模态任务，但设计复杂度高。
5. 适配器和投影层（Adapters & Projections）
原理：通过轻量级的适配器或投影层连接预训练好的单模态模型。
代表方法：

CLIP-Adapter：在CLIP上添加适配器层进行下游任务适应。
LLaVA：使用简单的投影层连接预训练的视觉模型和大型语言模型。

特点：参数高效，易于实现，但可能存在表达能力限制。
6. Q-Former架构
原理：使用查询(query)向量通过交叉注意力从视觉特征中提取信息，再与语言模型对齐。
代表方法：

BLIP-2：首创使用Q-Former架构连接视觉编码器和语言模型。
InstructBLIP：基于BLIP-2，进一步增强了指令跟随能力。

特点：结构新颖，模型参数量适中，但层级复杂。
Q-Former逐渐被替代的原因
Q-Former作为一种重要的跨模态对齐方法，在BLIP-2和InstructBLIP等模型中取得了显著成功。然而，最近的研究趋势显示它正被更简单有效的方法取代：
1. 架构复杂性
Q-Former包含多层交叉注意力机制和自注意力模块，结构较为复杂。相比之下，简单的线性投影层（如LLaVA）或MLP适配器（如CLIP-Adapter）更为简洁，易于实现和训练。
2. 效率问题
Q-Former的计算量相对较大，尤其是处理高分辨率图像或视频数据时。这种额外的计算开销在扩展到更大规模模型时可能成为瓶颈。
3. 缩小的性能差距
初期Q-Former相比简单投影层有明显优势，但随着预训练模型能力的提升和训练方法的改进，简单投影方法的性能已经接近甚至超过了Q-Former。例如，LLaVA-1.5仅使用简单的两层MLP就取得了很好的效果。
4. 参数效率
在模型参数相同的情况下，更简单的线性投影或MLP可以分配更多参数到真正需要学习的部分，而Q-Former的参数分布可能不够高效。
5. 端到端训练的趋势
最新研究更倾向于端到端训练模型，而Q-Former作为中间桥接层，增加了训练的复杂性。简单的投影层更容易与整体架构融合进行端到端优化。
6. 扩展性考虑
当扩展到多模态（如图像、视频、音频、3D等）时，Q-Former的通用性受限，而简单的投影层更容易为每种模态定制和扩展。




在分布式环境中使用vLLM的精妙策略：

收集所有进程的提示文本到主进程
只在主进程上运行vLLM生成（这是由于vLLM不支持分布式执行）
生成后，将完成内容广播回所有进程
每个进程只保留其负责的那部分完成内容


import torch
import torch.nn.functional as F
def manual_cross_entropy_loss(logits, targets):
	probabilities = F.softmax(logits, dim=1)
	log_probabilities = torch.log(probabilities + 1e-12)
	target_log_prob = log_probabilities.gather(dim=1, index=targets.unsqueeze(1)).squeeze(1)
	loss = -target_log_prob.mean()
	return loss

loss = - (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))
















